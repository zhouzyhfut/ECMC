# Voices, Faces, and Feelings: Multi-modal Emotion-Cognition Captioning for Mental Health Understanding *(AAAI 2026)*  
---

## ðŸŒŸ Overview  
Understanding mental health disorders requires integrating both **emotional** and **cognitive** factors. However, existing multi-modal approaches often frame the task as simple classification, leading to limited interpretabilityâ€”especially regarding the underlying emotional and cognitive mechanisms.  

To address these challenges, we propose **ECMC (Emotionâ€“Cognition Multi-modal Captioning)**, a novel framework designed to generate **natural language descriptions** of emotional and cognitive states from **audio, visual, and textual** inputs. ECMC employs an **encoderâ€“decoder architecture**, where modality-specific encoders are connected through a **dual-stream BridgeNet** based on Q-former. This design enables fine-grained emotionâ€“cognition feature fusion, further enhanced via **contrastive learning**. A **LLaMA-based decoder** then aligns these features with annotated captions to produce interpretable emotionâ€“cognition descriptions and profiles.  

Extensive objective and subjective evaluations demonstrate that ECMC not only **outperforms existing multi-modal LLMs and mental health models** in caption generation, but also **enhances diagnostic interpretability** by bridging perceptual cues and psychological reasoning.

